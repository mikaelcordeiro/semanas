{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semana 4 do Aceleradev DS Codenation\n",
    "\n",
    "### Professor: Túlio Vieira de Souza | Tema: Intervalos de Confiança, Testes de Hipóteses, P-Valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as sct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervalo de Confiança\n",
    "\n",
    "$$\\overline{x}-e_0\\leq\\mu\\leq\\overline{x}+e_0$$\n",
    "\n",
    "Como sempre, nunca fazemos um Censo para ter acesso a **toda** população. Gasta muito. E com uma amostra bem feita (não perfeita, mas Representativa), podemos inferir algum parâmetro populacional. Entretanto, devemos ter a percepção que todo parâmetro estimado de uma estatística não pode ser **pontual**, deve estar contido num intervalo, justamente porque analisamos uma amostra e queremos extrapolar seu valor para a população, mas uma população contém inumeras amostras diferentes, e como é impossível escolher uma amostra perfeita, sempre existirá um erro amostral na hora de escolha aleatória dos indivíduos dessa amostra. Este erro é chamado **Erro Amostral Aleatório**, que pode ser visto como a diferente entre o parâmetro populacional e a estatística. Exemplo:\n",
    "\n",
    "\n",
    "Escolho uma amostra de $n$ indivíduos e encontro $\\overline{x}_1$. Escolho outros indivíduos e, novamente, encontro a média $\\overline{x}_2$. Essas médias provavelmente serão diferentes. Portanto, $\\mu$ inferido por elas vai variar também. Outra vez: só se sabe o $\\mu$ exato quando faz um censo.\n",
    "\n",
    "\n",
    "Esse intervalo é chamado **Intervalo de Confiança**. Ele exprime onde os valores das médias poderão variar. Maaaas, mesmo tendo essa faixa de valores, ainda é possível que apareçam valores fora desse limite. Ou seja, existe uma *chance* da estatística estar contida no intervalo. E que probabilidade é essa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nível de Confiança ou Grau de Confiança\n",
    "\n",
    "$$1-\\alpha$$ sendo $\\alpha$ o **grau de significância**\n",
    "\n",
    "Essa medida diz a probabilidade de $\\mu$ estar contida num determinado intervalo. Geralmente, esse valor é de $1-\\alpha=95\\%$. Ou seja: $95\\%$ das amostras terão a média **dentro** do intervalo de confiança. Exemplo:\n",
    "\n",
    "Dada de uma população, eu seleciono algumas amostras de 20 indivíduos e calculo $\\overline{x}$ de cada uma. Com $1-\\alpha=0.95$, significa que: de 100 amostras com 20 indivíduos, 95 delas terão média dentro do intervalo de confiança, e outras 5 amostras terão média fora desse intervalo.\n",
    "\n",
    "Perceba: quanto maior o $1-\\alpha$, maior deve ser o intervalo de confiança, uma vez que a chance de conter $\\mu$ será maior. Por outro lado, quanto menor for o nível de confiança, mais estreito será esse intervalo, justamente pois a chance de conter $\\mu$ é menor.\n",
    "\n",
    "**Relação Intervalo de Confiança - Grau de Confiança - Grau de Significância**\n",
    "\n",
    "Como já sabemos quem é quem, qual é a probabilidade de um $\\overline{x}$ estar dentro do intervalo de confiança?\n",
    "\n",
    "$$P(\\overline{x}-e_0 \\leq \\mu \\leq \\overline{x}+e_0)=1-\\alpha$$\n",
    "\n",
    "E a probabilidade de estar fora?\n",
    "\n",
    "$$P(\\mu \\leq -e_0 \\cap \\mu \\geq e_0)=\\alpha$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes de Hipóteses\n",
    "\n",
    "Dados em si não dizem nada, mas a interpretação deles sim. Também chamados de *Testes de Significância*, os testes de hipóteses dão rigor estatístico às nossas crenças. Com eles, calculamos uma certa *quantidade* sobre uma dada amostra, e essa quantidade nos diz se o que acreditamos não se rejeita para toda a população, ou se rejeita. Essas hipóteses são as crenças iniciais que estão na mente da pessoa antes de se estudar o caso, e testar tais hipóteses significa ver qual delas é melhor sustentada estatísticamente.\n",
    "\n",
    "Para gerar um teste, a primeira coisa é definir as hipoteses:\n",
    "\n",
    "- H0: Hipótese Nula -> suposição que o teste se mantém. Não se rejeita à certo grau de significancia\n",
    "\n",
    "- H1: Hioótese Alternativa -> suposição que o teste não se mantém. Com isso, rejeita-se a hipótese nula à certo grau de significancia\n",
    "\n",
    "**Nota:** A hipótese nula é vista como uma *não mudança*, *falta de mudança*, *efeito nenhum*, ...\n",
    "\n",
    "***Aceitar* ou *Deixar de Rejeitar***\n",
    "\n",
    "Quando falamos *Aceitar* a hipótese nula, entende-se que ela é verdade. Mas, lembre-se: um teste de hipóteses lida com *eventos probabilísicos*. Ou seja, **não sabemos o que é verdadeiro ou falso**. Sabemos o que é provável ou não. Portanto, *deixar de rejeitar* é mais correto, pois significa que não existem evidências estatísticas suficientes para rejeitar a hipótese nula. Por conseguinte, quando rejeitamos, é porque existem evidências estatísticas sufcientes para rejeitar a hipótese nula, e que a diferença entre a estatísca amostral e o parâmetro populacional é *significativa*\n",
    "\n",
    "**Como interpretar o Teste de Hipóteses**\n",
    "\n",
    "Para que se tenha como *deixar de rejeitar* ou *rejeitar* a H0, devemos fazer uma **comparação**, e ela pode ser feita de duas maneiras diferentes:\n",
    "\n",
    "- usando *p-valor*\n",
    "\n",
    "- usando valor crítico\n",
    "\n",
    "E se compara tais medidas **DEPOIS** de se escolher o grau de confiança, pois através dele que obtemos o grau de significancia. Isso é feito porque muitas vezes os valores a serem comparados são muito próximos, e escolher o grau de confiança depois de obter o *p-valor*, ou *valor crítico*, pode denotar a conveniência de se gerar um resultado favorável propositalmente. E a escolha desse limite é baseada no que se está estudando.\n",
    "\n",
    "***P-valor***\n",
    "\n",
    "Esse valor é comparado com o $\\alpha$, e as conclusões são as seguintes\n",
    "\n",
    "Rejeita-se H0:\n",
    "\n",
    "$$p_{valor} \\leq \\alpha$$\n",
    "\n",
    "    Deixa-se de rejeitar a hipótese nula, com grau de significância de x%\n",
    "    \n",
    "    ou\n",
    "    \n",
    "    Deixa-se de rejeitar a hipótese nula, com grau de confiança de 1 - x%\n",
    "    \n",
    "Deixa-se de rejeitar H0:\n",
    "\n",
    "$$p_{valor}>\\alpha$$\n",
    "\n",
    "    Rejeita-se a hipótese nula, com grau de significância de x%\n",
    "    \n",
    "    ou\n",
    "    \n",
    "    Rejeita-se a hipótese nula, com grau de confiança de 1 - x%\n",
    "    \n",
    "O $p_{valor}$ indica o nível de compatibilidade entre os dados e a hipótese nula H0. Significa qual a porcentagem das vezes será possível ver uma dada $\\overline{x}$, ou maior (mais extrema), se a hipótese nula é verdadeira\n",
    "\n",
    "- $p_{valor}$ alto: os dados amostrais são muito compatíveis com uma hipótese nula verdadeira\n",
    "\n",
    "- $p_{valor}$ baixo: os dados amostrais não são muito compatíveis com uma hipótese nula verdadeira\n",
    "\n",
    "Exemplo:\n",
    "    \n",
    "    Imagine que a média populacional de consumo de gasolina no ano anterior foi de 260 litros por família. Para constatar se esse ano essa média mudou, podemos fazer uma amostra, com n famílias, e encontar a média amostral, 330 litros por família por exemplo. Com isso, encontramos o p-valor dessa amostra de 0.03112. Diante disso, a interpretação correta do p-valor diz que \"se a hipótese nula é verdadeira, obteremos médias amostrais de 330 litros por familia, ou maiores, em 3.112% das amostras, isso devido ao Erro Amostral Aleatório\"\n",
    "\n",
    "Agora, fica fácil entender porquê um $p_{valor}> \\alpha$ (valor alto) deixa-se de rejeitar H0. Porque, para a hipótese nula sendo verdade, o valor amostral pode ocorrer várias vezes.\n",
    "\n",
    "***Valor Crítico***\n",
    "\n",
    "Esse valor crítico é comparado ao valor crítico associado ao grau de significancia $\\alpha$\n",
    "\n",
    "Rejeita-se H0:\n",
    "\n",
    "$$|Z_{crit}|\\geq |Z(\\alpha)|$$\n",
    "\n",
    "    Rejeita-se a hipótese nula, com grau de significância de x%\n",
    "    \n",
    "    ou\n",
    "    \n",
    "    Rejeita-se a hipótese nula, com grau de confiança de 1 - x%\n",
    "\n",
    "Deixa-se de rejeitar H0:\n",
    "\n",
    "$$|Z_{crit}| < |Z(\\alpha)|$$\n",
    "\n",
    "    Deixa-se de rejeitar a hipótese nula, com grau de significância de x%\n",
    "    \n",
    "    ou\n",
    "    \n",
    "    Deixa-se de rejeitar a hipótese nula, com grau de confiança de 1 - x%\n",
    "\n",
    "*obs.:*só usei Z para remeter à dados padronizados da tabela *escores-z*, mas poderia ser t da tabela de *t de Student*\n",
    "\n",
    "Para valores críticos, fica um pouco mais claro que a comparação não representa uma probabilidade, e sim quão compatível o conjunto de dados é com a hipótese nula H0. Para tanto, as mesmas condições de interpretação do *p-valor* servem para *valores críticos*\n",
    "\n",
    "***Erros da Estatística de Teste***\n",
    "\n",
    "- Erro Tipo I (Falso Positivo): Rejeição de uma hipótese nula verdadeira, com $\\alpha \\%$ de chance de cometer esse erro\n",
    "\n",
    "- Erro Tipo II (Falso Negativo): Não rejeição de uma hipótese nula falsa\n",
    "\n",
    "Para entender melhor:\n",
    "\n",
    "    Se o grau de significancia for de 5%, pode ser que, em 20 hipóteses nulas, 1 seja rejeitada, ou deixada de ser rejeitada, incorretamente, devido ao ruído do conjunto de dados.\n",
    "    \n",
    "    Falso Positivo: imagine um p-valor baixo (rejeita-se H0). Nesse caso, pode ser que H0 seja falso mesmo, que existiam evidencias estatísticas suficientes para a rejeição, ooooou H0 é verdadeiro e algum evento raro e incomum foi observado. Nessa situação, rejeitar H0 é um erro, pois acreditou erroneamente na rejeição\n",
    "    \n",
    "    Falso Negativo: imagine um p-valor alto (deixa-se de rejeitar H0). Nesse caso, pode ser que H0 seja verdadeiro, que não existiam evidências estatísticas suficientes para rejeição, oooouu H0 é falso e algum evento raro e incomum foi observado. Nessa situação, deixar de rejeitar é um erro, pois acreditou-se erroneamente em deixar de rejeitar.\n",
    "\n",
    "todo teste de hipóteses está sujeito a um desses erros. Eles são prováveis de ocorrentem.\n",
    "\n",
    "Por fim, a idéia que esses erros tentam levantar é que: temos uma hipótese sobre uma população, e geralmente coletamos amostras sobre ela para fazer um teste (caso a populaçao seja grande ou inacessível). Devido ao $\\alpha$, podemos escolher amostras fora do Intervalo de Confiança (o que vimos que, mesmo pouco provável, pode acontecer devido ao Erro Amostral Aleatório), e para a estatística de teste, essa amostra será inusitada por ser muito distante da maioria dos valores. Ou seja, podemos rejeitar uma hipótese verdadeira (Erro Tipo I) pois o $p_{valor}$ dessa amostra é $\\leq \\alpha$. Mas, se pegassemos qualquer outra amostra dentro do $1-\\alpha$, elas retornariam um $p_{valor}>\\alpha$. Agora, podemos deixar de rejeitar uma hipótese nula falsa (Erro Tipo II), caso nossa amostra retorne um $p_{valor}>\\alpha$, mas ela nos fez fazer isso pois é inusitada. Agora, se pegassemos uma amostra dentro do Intervalo de Confiança, ela nos daria um $p_{valor}\\leq \\alpha$, o que nos faria rejeitar uma hipótese nula falsa, o que estaria certo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
